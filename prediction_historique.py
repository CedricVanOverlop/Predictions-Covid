"""
Modification du mod√®le de pr√©diction pour validation historique
Utilise la p√©riode Mars-Juin 2021 pour comparer pr√©dictions vs r√©alit√©
"""

import json
import os
import numpy as np
import datetime
from typing import Dict, List, Tuple, Optional
from colorama import init, Fore

# Initialisation de colorama
init(autoreset=True)


class HistoricalValidationModel:
    """
    Mod√®le de validation sur donn√©es historiques Mars-Juin 2021
    """
    
    def __init__(self, smoothed_data_file: str = "data/smoothed_data.json", 
                 geographic_weights_file: str = "data/geographic_weights.json"):
        """
        Initialise le mod√®le pour validation historique
        """
        print("üî¢ Initialisation du mod√®le de validation historique...")
        
        # Chargement des donn√©es
        self.smoothed_data = self._load_smoothed_data(smoothed_data_file)
        self.geographic_weights = self._load_geographic_weights(geographic_weights_file)
        
        # Communes (ordre fixe pour la matrice)
        self.communes = sorted(list(self.geographic_weights.keys()))
        self.n_communes = len(self.communes)
        self.commune_to_index = {commune: i for i, commune in enumerate(self.communes)}
        
        # P√©riodes d√©finies
        self.training_start = "2021-01-01"
        self.training_end = "2021-02-28"
        self.validation_start = "2021-03-01"
        self.validation_end = "2021-06-30"
        
        print(f"‚úÖ Mod√®le initialis√© pour validation historique")
        print(f"üìÖ Entra√Ænement : {self.training_start} ‚Üí {self.training_end}")
        print(f"üìÖ Validation : {self.validation_start} ‚Üí {self.validation_end}")
    
    def _load_smoothed_data(self, file_path: str) -> Dict:
        """Charge les donn√©es liss√©es"""
        try:
            with open(file_path, 'r', encoding='utf8') as f:
                data = json.load(f)
            
            if "data" in data:
                return data["data"]
            else:
                return data
                
        except FileNotFoundError:
            print(Fore.RED + f"‚ùå Fichier non trouv√© : {file_path}")
            raise
    
    def _load_geographic_weights(self, file_path: str) -> Dict[str, Dict[str, float]]:
        """Charge les poids g√©ographiques"""
        try:
            with open(file_path, 'r', encoding='utf8') as f:
                data = json.load(f)
            
            if "weights" in data:
                return data["weights"]
            else:
                return data
                
        except FileNotFoundError:
            print(Fore.RED + f"‚ùå Fichier non trouv√© : {file_path}")
            raise
    
    def extract_period_data(self, start_date: str, end_date: str) -> Tuple[List[str], np.ndarray]:
        """
        Extrait les donn√©es pour une p√©riode donn√©e
        
        Returns:
            dates: Liste des dates
            data_matrix: Matrice [communes √ó jours]
        """
        period_dates = []
        period_data = []
        
        for date in sorted(self.smoothed_data.keys()):
            if start_date <= date <= end_date:
                period_dates.append(date)
                
                # Extraction des cas pour toutes les communes √† cette date
                day_data = []
                for commune in self.communes:
                    cases = self.smoothed_data[date].get(commune, 0.0)
                    day_data.append(float(cases))
                
                period_data.append(day_data)
        
        if not period_data:
            raise ValueError(f"Aucune donn√©e trouv√©e pour la p√©riode {start_date} - {end_date}")
        
        # Conversion en matrice numpy [communes √ó jours]
        data_matrix = np.array(period_data).T
        
        print(f"üìä P√©riode {start_date} - {end_date} : {len(period_dates)} jours, {data_matrix.shape[0]} communes")
        
        return period_dates, data_matrix
    
    def train_model_on_period(self, alpha_geo: float = 0.5) -> np.ndarray:
        """
        Entra√Æne le mod√®le de Markov sur la p√©riode d'entra√Ænement
        
        Args:
            alpha_geo: Param√®tre de pond√©ration g√©ographique
            
        Returns:
            Matrice de transition [19√ó19]
        """
        print(f"üéØ Entra√Ænement du mod√®le sur {self.training_start} - {self.training_end}...")
        
        # Extraction des donn√©es d'entra√Ænement
        train_dates, train_matrix = self.extract_period_data(self.training_start, self.training_end)
        
        if train_matrix.shape[1] < 2:
            raise ValueError("Pas assez de donn√©es pour l'entra√Ænement")
        
        # Pr√©paration des matrices X(t) et X(t+1)
        X_t = train_matrix[:, :-1]   # [19 √ó (T-1)]
        X_t1 = train_matrix[:, 1:]   # [19 √ó (T-1)]
        
        print(f"üìà Matrices d'entra√Ænement : {X_t.shape}")
        
        # Estimation de la matrice de base par moindres carr√©s
        try:
            XtXt_T = X_t @ X_t.T
            regularization = 1e-6 * np.eye(self.n_communes)
            XtXt_T_reg = XtXt_T + regularization
            XtXt_T_inv = np.linalg.inv(XtXt_T_reg)
            A_base = X_t1 @ X_t.T @ XtXt_T_inv
        except np.linalg.LinAlgError:
            print(Fore.YELLOW + "‚ö†Ô∏è Utilisation de la pseudo-inverse")
            A_base = X_t1 @ np.linalg.pinv(X_t)
        
        # Construction de la matrice g√©ographique
        geo_matrix = np.zeros((self.n_communes, self.n_communes))
        for i, commune_i in enumerate(self.communes):
            for j, commune_j in enumerate(self.communes):
                if commune_i in self.geographic_weights:
                    weight = self.geographic_weights[commune_i].get(commune_j, 0.0)
                    geo_matrix[i, j] = weight
        
        # Application des contraintes g√©ographiques
        A_geographic = A_base * geo_matrix
        A_final = (1 - alpha_geo) * A_base + alpha_geo * A_geographic
        
        print(f"‚úÖ Mod√®le entra√Æn√© avec Œ±_geo = {alpha_geo}")
        
        return A_final
    
    def predict_validation_period(self, transition_matrix: np.ndarray) -> Tuple[List[str], np.ndarray, np.ndarray]:
        """
        Effectue des pr√©dictions sur la p√©riode de validation
        
        Args:
            transition_matrix: Matrice de transition entra√Æn√©e
            
        Returns:
            dates: Dates de validation
            real_data: Donn√©es r√©elles [communes √ó jours]
            predictions: Pr√©dictions [communes √ó jours]
        """
        print(f"üîÆ Pr√©dictions sur {self.validation_start} - {self.validation_end}...")
        
        # Donn√©es r√©elles de validation
        val_dates, real_data = self.extract_period_data(self.validation_start, self.validation_end)
        
        # √âtat initial : derni√®re observation de la p√©riode d'entra√Ænement
        _, train_data = self.extract_period_data(self.training_start, self.training_end)
        initial_state = train_data[:, -1].reshape(-1, 1)  # Derni√®re colonne comme √©tat initial
        
        print(f"üöÄ √âtat initial : {np.sum(initial_state):.1f} cas totaux")
        
        # G√©n√©ration des pr√©dictions jour par jour
        predictions = np.zeros((self.n_communes, len(val_dates)))
        current_state = initial_state.copy()
        
        for day in range(len(val_dates)):
            predictions[:, day] = current_state.flatten()
            
            # Pr√©diction du jour suivant avec la matrice de Markov
            if day < len(val_dates) - 1:
                next_state = transition_matrix @ current_state
                current_state = np.maximum(next_state, 0)  # √âviter les valeurs n√©gatives
        
        print(f"‚úÖ {len(val_dates)} jours de pr√©dictions g√©n√©r√©es")
        
        return val_dates, real_data, predictions
    
    def evaluate_predictions(self, real_data: np.ndarray, predictions: np.ndarray) -> Dict:
        """
        √âvalue la qualit√© des pr√©dictions
        
        Args:
            real_data: Donn√©es r√©elles [communes √ó jours]
            predictions: Pr√©dictions [communes √ó jours]
            
        Returns:
            Dictionnaire avec les m√©triques d'√©valuation
        """
        print("üìä √âvaluation des pr√©dictions...")
        
        # M√©triques globales
        mae_global = np.mean(np.abs(real_data - predictions))
        rmse_global = np.sqrt(np.mean((real_data - predictions) ** 2))
        
        # M√©triques par commune
        mae_by_commune = {}
        correlation_by_commune = {}
        
        for i, commune in enumerate(self.communes):
            real_series = real_data[i, :]
            pred_series = predictions[i, :]
            
            mae_commune = np.mean(np.abs(real_series - pred_series))
            
            # Corr√©lation (si variance non nulle)
            if np.var(real_series) > 1e-6 and np.var(pred_series) > 1e-6:
                correlation = np.corrcoef(real_series, pred_series)[0, 1]
            else:
                correlation = 0.0
            
            mae_by_commune[commune] = mae_commune
            correlation_by_commune[commune] = correlation
        
        # M√©triques temporelles (√©volution jour par jour)
        daily_errors = np.mean(np.abs(real_data - predictions), axis=0)
        
        results = {
            "mae_global": mae_global,
            "rmse_global": rmse_global,
            "mae_by_commune": mae_by_commune,
            "correlation_by_commune": correlation_by_commune,
            "daily_errors": daily_errors.tolist(),
            "best_commune": min(mae_by_commune.items(), key=lambda x: x[1]),
            "worst_commune": max(mae_by_commune.items(), key=lambda x: x[1])
        }
        
        print(f"üìà MAE globale : {mae_global:.2f}")
        print(f"üìà RMSE globale : {rmse_global:.2f}")
        print(f"üèÜ Meilleure commune : {results['best_commune'][0]} (MAE = {results['best_commune'][1]:.2f})")
        print(f"üîª Moins bonne commune : {results['worst_commune'][0]} (MAE = {results['worst_commune'][1]:.2f})")
        
        return results
    
    def optimize_alpha_on_validation(self, alpha_values: List[float] = None) -> Tuple[float, Dict]:
        """
        Optimise le param√®tre alpha sur les donn√©es de validation
        
        Args:
            alpha_values: Liste des valeurs d'alpha √† tester
            
        Returns:
            Meilleur alpha et r√©sultats d√©taill√©s
        """
        if alpha_values is None:
            alpha_values = [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]
        
        print(f"üéØ Optimisation d'alpha sur {len(alpha_values)} valeurs...")
        
        results_by_alpha = {}
        best_alpha = None
        best_mae = float('inf')
        
        for alpha in alpha_values:
            print(f"\nüîß Test Œ± = {alpha}...")
            
            # Entra√Ænement avec cet alpha
            transition_matrix = self.train_model_on_period(alpha_geo=alpha)
            
            # Pr√©dictions
            val_dates, real_data, predictions = self.predict_validation_period(transition_matrix)
            
            # √âvaluation
            evaluation = self.evaluate_predictions(real_data, predictions)
            
            results_by_alpha[alpha] = {
                "transition_matrix": transition_matrix.tolist(),
                "evaluation": evaluation,
                "val_dates": val_dates,
                "real_data": real_data.tolist(),
                "predictions": predictions.tolist()
            }
            
            # Mise √† jour du meilleur mod√®le
            if evaluation["mae_global"] < best_mae:
                best_mae = evaluation["mae_global"]
                best_alpha = alpha
        
        print(f"\nüèÜ Meilleur mod√®le : Œ± = {best_alpha} (MAE = {best_mae:.2f})")
        
        return best_alpha, results_by_alpha
    
    def save_historical_validation_results(self, best_alpha: float, results_by_alpha: Dict, 
                                         output_file: str = "data/historical_validation_results.json"):
        """
        Sauvegarde les r√©sultats de validation historique
        """
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        output_data = {
            "metadata": {
                "created_at": datetime.datetime.now().isoformat(),
                "model_type": "historical_validation_markov",
                "training_period": f"{self.training_start} to {self.training_end}",
                "validation_period": f"{self.validation_start} to {self.validation_end}",
                "best_alpha": best_alpha,
                "communes": self.communes
            },
            "results": results_by_alpha
        }
        
        with open(output_file, 'w', encoding='utf8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ R√©sultats sauvegard√©s : {output_file}")
        
        return output_file
    
    def generate_comparison_data_for_visualization(self, best_alpha: float, results_by_alpha: Dict) -> Dict:
        """
        G√©n√®re les donn√©es format√©es pour la visualisation dans main.py
        """
        print("üìä G√©n√©ration des donn√©es pour visualisation...")
        
        best_results = results_by_alpha[best_alpha]
        
        # Conversion des donn√©es pour compatibilit√© avec main.py
        val_dates = best_results["val_dates"]
        real_data = np.array(best_results["real_data"])
        predictions = np.array(best_results["predictions"])
        
        # Format compatible avec main.py
        comparison_data = {
            "metadata": {
                "validation_period": f"{self.validation_start} to {self.validation_end}",
                "best_alpha": best_alpha,
                "mae_global": best_results["evaluation"]["mae_global"],
                "communes": self.communes
            },
            "dates": val_dates,
            "real_data": {},
            "predictions": {},
            "metrics": best_results["evaluation"]
        }
        
        # Organisation par commune
        for i, commune in enumerate(self.communes):
            comparison_data["real_data"][commune] = real_data[i, :].tolist()
            comparison_data["predictions"][commune] = predictions[i, :].tolist()
        
        # Sauvegarde pour main.py
        with open("data/historical_comparison_data.json", 'w', encoding='utf8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)
        
        print("üíæ Donn√©es de comparaison sauvegard√©es : data/historical_comparison_data.json")
        
        return comparison_data


def run_historical_validation():
    """
    Fonction principale pour ex√©cuter la validation historique
    """
    print("üöÄ D√©marrage de la validation historique Mars-Juin 2021")
    print("="*60)
    
    try:
        # Initialisation du mod√®le
        model = HistoricalValidationModel()
        
        # Optimisation d'alpha
        best_alpha, results_by_alpha = model.optimize_alpha_on_validation()
        
        # Sauvegarde des r√©sultats
        model.save_historical_validation_results(best_alpha, results_by_alpha)
        
        # G√©n√©ration des donn√©es pour visualisation
        comparison_data = model.generate_comparison_data_for_visualization(best_alpha, results_by_alpha)
        
        print("\n" + "="*60)
        print("‚úÖ Validation historique termin√©e avec succ√®s !")
        print(f"üèÜ Meilleur mod√®le : Œ± = {best_alpha}")
        print(f"üìä MAE globale : {comparison_data['metadata']['mae_global']:.2f}")
        print("üìÅ R√©sultats disponibles dans 'data/'")
        
        return comparison_data
        
    except Exception as e:
        print(Fore.RED + f"‚ùå Erreur lors de la validation : {e}")
        raise


def test_historical_validation():
    """Test de la validation historique"""
    print("üß™ Test de la validation historique...")
    
    try:
        comparison_data = run_historical_validation()
        
        # Affichage d'exemples de r√©sultats
        print(f"\nüìä Exemple de r√©sultats pour Bruxelles :")
        if "Bruxelles" in comparison_data["real_data"]:
            real_brussels = comparison_data["real_data"]["Bruxelles"][:5]
            pred_brussels = comparison_data["predictions"]["Bruxelles"][:5]
            dates_sample = comparison_data["dates"][:5]
            
            for i, date in enumerate(dates_sample):
                print(f"   {date}: R√©el = {real_brussels[i]:.1f}, Pr√©dit = {pred_brussels[i]:.1f}")
        
        print("‚úÖ Test termin√© avec succ√®s !")
        
    except Exception as e:
        print(Fore.RED + f"‚ùå Erreur dans le test : {e}")
        raise


if __name__ == "__main__":
    test_historical_validation()